{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c0ff441ab9134c26bf11e280c7e36929":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2020e5f3c5241da84df29fa2c1b4d16","IPY_MODEL_342ee8b0016541c682a123257e101e63","IPY_MODEL_345d7222fd6d4f95833669ffe5952014"],"layout":"IPY_MODEL_036111dd881d4a8fab3b79d2db3acdaf"}},"f2020e5f3c5241da84df29fa2c1b4d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6038dd288bf4816ae9900d6c73a9e65","placeholder":"​","style":"IPY_MODEL_ea04e013b1f642dc870de1df04c57ed5","value":"sampling loop time step: 100%"}},"342ee8b0016541c682a123257e101e63":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53c633cb93124bc0b0810dfd0b24baf5","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8359e85df3064a19b9972efb0c643a82","value":500}},"345d7222fd6d4f95833669ffe5952014":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_368f40afa84b4790bda61731a7a41d73","placeholder":"​","style":"IPY_MODEL_aea443db2f844397850dab7678420f0f","value":" 500/500 [07:17&lt;00:00,  1.14it/s]"}},"036111dd881d4a8fab3b79d2db3acdaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6038dd288bf4816ae9900d6c73a9e65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea04e013b1f642dc870de1df04c57ed5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53c633cb93124bc0b0810dfd0b24baf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8359e85df3064a19b9972efb0c643a82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"368f40afa84b4790bda61731a7a41d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aea443db2f844397850dab7678420f0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64978c5781204244b472f6ace8562ee1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_318d9d7d551749e6a0f39619df0c1b84","IPY_MODEL_558d490f6d2c4c8b8e000b530461de9e","IPY_MODEL_bff07d1462c641b3a0c72e64160f591d"],"layout":"IPY_MODEL_7a7c8dc2e73a4c7b939edb0cef37f087"}},"318d9d7d551749e6a0f39619df0c1b84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d41c72c05cb480ca046db24a4934527","placeholder":"​","style":"IPY_MODEL_ac84cc26cd04421bbcddb7b5ad394f7e","value":"sampling loop time step: 100%"}},"558d490f6d2c4c8b8e000b530461de9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c77e255a1755413b8606be67c1d4f7c4","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c9542868fd8c49e68cd0830234a24e8f","value":500}},"bff07d1462c641b3a0c72e64160f591d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3483424cb54909836b83e27c9c017b","placeholder":"​","style":"IPY_MODEL_0e105386196d47f3bf67ac882f3348b2","value":" 500/500 [07:16&lt;00:00,  1.15it/s]"}},"7a7c8dc2e73a4c7b939edb0cef37f087":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d41c72c05cb480ca046db24a4934527":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac84cc26cd04421bbcddb7b5ad394f7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c77e255a1755413b8606be67c1d4f7c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9542868fd8c49e68cd0830234a24e8f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc3483424cb54909836b83e27c9c017b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e105386196d47f3bf67ac882f3348b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78df1721cf6b4f40b00844d24b648a94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb0aa36f515747edbd1d9176da550698","IPY_MODEL_5b81d1dc66e841528003ca25b563b129","IPY_MODEL_be31d2a4f22b4a6e934c17bd54c9d30f"],"layout":"IPY_MODEL_54368585fc164f24bb0d833f9c436057"}},"fb0aa36f515747edbd1d9176da550698":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e74fb8135c7d4f6dbeaadab046b8768a","placeholder":"​","style":"IPY_MODEL_0880bb1992b740b7bbd4b4e1ddaa2853","value":"sampling loop time step: 100%"}},"5b81d1dc66e841528003ca25b563b129":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8b016a35a1b47c0879ac10874de5f16","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3a961ff074949e3a51ba29c146c590a","value":500}},"be31d2a4f22b4a6e934c17bd54c9d30f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7110474a9f194164b25633f73ec87617","placeholder":"​","style":"IPY_MODEL_bc0ddf2963684a328613cdcbd8f3741e","value":" 500/500 [07:15&lt;00:00,  1.15it/s]"}},"54368585fc164f24bb0d833f9c436057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e74fb8135c7d4f6dbeaadab046b8768a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0880bb1992b740b7bbd4b4e1ddaa2853":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8b016a35a1b47c0879ac10874de5f16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3a961ff074949e3a51ba29c146c590a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7110474a9f194164b25633f73ec87617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0ddf2963684a328613cdcbd8f3741e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1c65c89cae14d8fb021158738d6b752":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_671b10fb3e754dd393068e45bb890b00","IPY_MODEL_9a5d9fff41c34de6ac4cebc82c466d4b","IPY_MODEL_fe520552053a4d01aabc5aee75a09fe7"],"layout":"IPY_MODEL_dd54e51c3e1e487ea683d75ecf0dc798"}},"671b10fb3e754dd393068e45bb890b00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19cf34230bf046aa9fa2873809ce08f8","placeholder":"​","style":"IPY_MODEL_dc325ef90bb04c37a9cec6fb7ef46b36","value":"sampling loop time step:   5%"}},"9a5d9fff41c34de6ac4cebc82c466d4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e835ab8b0764882a0a634ff4b2077e9","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_086472ea8a644f32b80008635751677e","value":27}},"fe520552053a4d01aabc5aee75a09fe7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9c751554c2a49d3858e9643e5925260","placeholder":"​","style":"IPY_MODEL_a84156edf2514f9cbd08e403691c85eb","value":" 27/500 [00:22&lt;06:51,  1.15it/s]"}},"dd54e51c3e1e487ea683d75ecf0dc798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19cf34230bf046aa9fa2873809ce08f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc325ef90bb04c37a9cec6fb7ef46b36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e835ab8b0764882a0a634ff4b2077e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"086472ea8a644f32b80008635751677e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9c751554c2a49d3858e9643e5925260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a84156edf2514f9cbd08e403691c85eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# HW6 Diffusion Model\n\n**Sources:**\n- Github implementation [Denoising Diffusion Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n- Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)\n","metadata":{"id":"HhIgGq3za0yh"}},{"cell_type":"markdown","source":"## Import Packages and Set Seeds","metadata":{"id":"wLHSIArLcFK0"}},{"cell_type":"code","source":"!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n!apt-get install git-lfs\n!git lfs install\n!git clone https://huggingface.co/datasets/LeoFeng/MLHW_6\n!unzip ./MLHW_6/faces.zip -d .\n# install required dependencies\n!pip install einops\n!pip install transformers\n!pip install ema_pytorch\n!pip install accelerate","metadata":{"id":"s1xegyILIuLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680094716996,"user_tz":-480,"elapsed":65563,"user":{"displayName":"伏宇寬","userId":"05448255760746177782"}},"outputId":"b71ce929-c0bd-4ff0-ff53-cb01b421c2e9","execution":{"iopub.status.busy":"2023-04-21T04:22:12.898475Z","iopub.execute_input":"2023-04-21T04:22:12.898937Z","iopub.status.idle":"2023-04-21T04:24:46.308631Z","shell.execute_reply.started":"2023-04-21T04:22:12.898900Z","shell.execute_reply":"2023-04-21T04:24:46.307338Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Detected operating system as Ubuntu/focal.\nChecking for curl...\nDetected curl...\nChecking for gpg...\nDetected gpg...\nDetected apt version as 2.0.9\nRunning apt-get update... done.\nInstalling apt-transport-https... done.\nInstalling /etc/apt/sources.list.d/github_git-lfs.list...done.\nImporting packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\ndone.\nRunning apt-get update... done.\n\nThe repository is setup! You can now install packages.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (3.3.0).\n0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\nGit LFS initialized.\nfatal: destination path 'MLHW_6' already exists and is not an empty directory.\nArchive:  ./MLHW_6/faces.zip\nreplace ./faces/70701.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\nRequirement already satisfied: einops in /opt/conda/lib/python3.7/site-packages (0.6.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: ema_pytorch in /opt/conda/lib/python3.7/site-packages (0.2.3)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from ema_pytorch) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->ema_pytorch) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.21.6)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (23.0)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport copy\nfrom pathlib import Path\nfrom random import random\nfrom functools import partial\nfrom collections import namedtuple\nfrom multiprocessing import cpu_count\n\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom torch.optim import Adam\n\nimport torchvision\nfrom torchvision import transforms as T, utils\n\nfrom einops import rearrange, reduce, repeat\nfrom einops.layers.torch import Rearrange\n\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom ema_pytorch import EMA\n\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt\nimport os\n\ntorch.backends.cudnn.benchmark = True\ntorch.manual_seed(4096)\n\nif torch.cuda.is_available():\n  torch.cuda.manual_seed(4096)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQnlc27k7Aiw","outputId":"122e675a-a91b-4fa3-d56f-1f6bb38bc094","executionInfo":{"status":"ok","timestamp":1680094720989,"user_tz":-480,"elapsed":4012,"user":{"displayName":"伏宇寬","userId":"05448255760746177782"}},"execution":{"iopub.status.busy":"2023-04-21T04:24:46.313333Z","iopub.execute_input":"2023-04-21T04:24:46.313677Z","iopub.status.idle":"2023-04-21T04:24:46.326143Z","shell.execute_reply.started":"2023-04-21T04:24:46.313643Z","shell.execute_reply":"2023-04-21T04:24:46.325016Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Forward process (Noise scheduler)\n\n\n","metadata":{"id":"Rj17psVw7Shg"}},{"cell_type":"code","source":"def linear_beta_schedule(timesteps):\n    \"\"\"\n    linear schedule, proposed in original ddpm paper\n    \"\"\"\n    scale = 1000 / timesteps\n    beta_start = scale * 0.0001\n    beta_end = scale * 0.02\n    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))","metadata":{"id":"qWw50ui9IZ5q","execution":{"iopub.status.busy":"2023-04-21T04:24:46.328920Z","iopub.execute_input":"2023-04-21T04:24:46.329302Z","iopub.status.idle":"2023-04-21T04:24:46.340848Z","shell.execute_reply.started":"2023-04-21T04:24:46.329258Z","shell.execute_reply":"2023-04-21T04:24:46.339874Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Create dataset","metadata":{"id":"Vt6JSKawk7_b"}},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size\n    ):\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for p in Path(f'{folder}').glob(f'**/*.jpg')]\n        #################################\n        ## TODO: Data Augmentation ##\n        #################################\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)","metadata":{"id":"uuckjpW_k1LN","execution":{"iopub.status.busy":"2023-04-21T04:24:46.344063Z","iopub.execute_input":"2023-04-21T04:24:46.344342Z","iopub.status.idle":"2023-04-21T04:24:46.354026Z","shell.execute_reply.started":"2023-04-21T04:24:46.344295Z","shell.execute_reply":"2023-04-21T04:24:46.352973Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: The backward process = U-Net\n\n","metadata":{"id":"buW6BaNga-XH"}},{"cell_type":"markdown","source":"Define some useful functions and U-Net","metadata":{"id":"iYw6u0nJXIWy"}},{"cell_type":"code","source":"def exists(x):\n    return x is not None\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef has_int_squareroot(num):\n    return (math.sqrt(num) ** 2) == num\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\n# normalization functions\n\ndef normalize_to_neg_one_to_one(img):\n    return img * 2 - 1\n\ndef unnormalize_to_zero_to_one(t):\n    return (t + 1) * 0.5\n\n# small helper modules\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\ndef Upsample(dim, dim_out = None):\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n    )\n\ndef Downsample(dim, dim_out = None):\n    return nn.Sequential(\n        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n    )\n\nclass WeightStandardizedConv2d(nn.Conv2d):\n    \"\"\"\n    https://arxiv.org/abs/1903.10520\n    weight standardization purportedly works synergistically with group normalization\n    \"\"\"\n    def forward(self, x):\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n\n        weight = self.weight\n        mean = reduce(weight, 'o ... -> o 1 1 1', 'mean')\n        var = reduce(weight, 'o ... -> o 1 1 1', partial(torch.var, unbiased = False))\n        normalized_weight = (weight - mean) * (var + eps).rsqrt()\n\n        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n\n    def forward(self, x):\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) * (var + eps).rsqrt() * self.g\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.fn(x)\n\n# sinusoidal positional embeds\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass RandomOrLearnedSinusoidalPosEmb(nn.Module):\n    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim, is_random = False):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n\n    def forward(self, x):\n        x = rearrange(x, 'b -> b 1')\n        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\n# building block modules\n\nclass Block(nn.Module):\n    def __init__(self, dim, dim_out, groups = 8):\n        super().__init__()\n        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding = 1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n\n    def forward(self, x, scale_shift = None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.act(x)\n        return x\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, dim_out * 2)\n        ) if exists(time_emb_dim) else None\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n\n    def forward(self, x, time_emb = None):\n\n        scale_shift = None\n        if exists(self.mlp) and exists(time_emb):\n            time_emb = self.mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, scale_shift = scale_shift)\n\n        h = self.block2(h)\n\n        return h + self.res_conv(x)\n\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, heads = 4, dim_head = 32):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(hidden_dim, dim, 1),\n            LayerNorm(dim)\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n\n        q = q.softmax(dim = -2)\n        k = k.softmax(dim = -1)\n\n        q = q * self.scale\n        v = v / (h * w)\n\n        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n\n        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n        return self.to_out(out)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 4, dim_head = 32):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n\n        q = q * self.scale\n\n        sim = torch.einsum('b h d i, b h d j -> b h i j', q, k)\n        attn = sim.softmax(dim = -1)\n        out = torch.einsum('b h i j, b h d j -> b h i d', attn, v)\n\n        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n        return self.to_out(out)\n\n# model\n\nclass Unet(nn.Module):\n    def __init__(\n        self,\n        dim,\n        init_dim = None,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        channels = 3,\n        resnet_block_groups = 8,\n        learned_sinusoidal_cond = False,\n        random_fourier_features = False,\n        learned_sinusoidal_dim = 16\n    ):\n        super().__init__()\n\n        # determine dimensions\n\n        self.channels = channels\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n\n        if self.random_or_learned_sinusoidal_cond:\n            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n            fourier_dim = learned_sinusoidal_dim + 1\n        else:\n            sinu_pos_emb = SinusoidalPosEmb(dim)\n            fourier_dim = dim\n\n        self.time_mlp = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(fourier_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n            ]))\n\n        self.out_dim = default(out_dim, channels)\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n\n    def forward(self, x, time):\n        x = self.init_conv(x)\n        r = x.clone()\n\n        t = self.time_mlp(time)\n\n        h = []\n\n        for block1, block2, attn, downsample in self.downs:\n            x = block1(x, t)\n            h.append(x)\n\n            x = block2(x, t)\n            x = attn(x)\n            h.append(x)\n\n            x = downsample(x)\n\n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t)\n\n        for block1, block2, attn, upsample in self.ups:\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block1(x, t)\n\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block2(x, t)\n            x = attn(x)\n\n            x = upsample(x)\n\n        x = torch.cat((x, r), dim = 1)\n\n        x = self.final_res_block(x, t)\n        return self.final_conv(x)\nmodel = Unet(64)","metadata":{"id":"DuJCCZ5dInQq","execution":{"iopub.status.busy":"2023-04-21T04:24:46.357462Z","iopub.execute_input":"2023-04-21T04:24:46.358126Z","iopub.status.idle":"2023-04-21T04:24:46.780058Z","shell.execute_reply.started":"2023-04-21T04:24:46.358087Z","shell.execute_reply":"2023-04-21T04:24:46.778973Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: The Diffusion Process\n\n","metadata":{"id":"8B9GlZrotBXy"}},{"cell_type":"markdown","source":"Define diffusion process, including generating noisy models, sample...\n","metadata":{"id":"ph05t8MxXMoY"}},{"cell_type":"code","source":"class GaussianDiffusion(nn.Module):\n    def __init__(\n        self,\n        model,\n        *,\n        image_size,\n        timesteps = 1000,\n        beta_schedule = 'linear',\n        auto_normalize = True\n    ):\n        super().__init__()\n        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n        assert not model.random_or_learned_sinusoidal_cond\n\n        self.model = model\n\n        self.channels = self.model.channels\n\n        self.image_size = image_size\n\n\n        if beta_schedule == 'linear':\n            beta_schedule_fn = linear_beta_schedule\n        else:\n            raise ValueError(f'unknown beta schedule {beta_schedule}')\n        \n        # calculate beta and other precalculated parameters\n        betas = beta_schedule_fn(timesteps)\n                                            \n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n\n        # sampling related parameters\n\n        self.sampling_timesteps = timesteps # default num sampling timesteps to number of timesteps at training\n\n        # helper function to register buffer from float64 to float32\n\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)\n        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n\n        register_buffer('posterior_variance', posterior_variance)\n\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n\n        # derive loss weight\n        # snr - signal noise ratio\n\n        snr = alphas_cumprod / (1 - alphas_cumprod)\n\n        # https://arxiv.org/abs/2303.09556\n\n        maybe_clipped_snr = snr.clone()\n\n        register_buffer('loss_weight', maybe_clipped_snr / snr)\n\n        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n\n        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def model_predictions(self, x, t, clip_x_start = False, rederive_pred_noise = False):\n        model_output = self.model(x, t)\n        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n\n        pred_noise = model_output\n        x_start = self.predict_start_from_noise(x, t, pred_noise)\n        x_start = maybe_clip(x_start)\n\n        if clip_x_start and rederive_pred_noise:\n            pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        return pred_noise, x_start\n\n    def p_mean_variance(self, x, t, clip_denoised = True):\n        noise, x_start = self.model_predictions(x, t)\n\n        if clip_denoised:\n            x_start.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n        return model_mean, posterior_variance, posterior_log_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(self, x, t: int):\n        b, *_, device = *x.shape, x.device\n        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, clip_denoised = True)\n        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n        return pred_img, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_all_timesteps = False):\n        batch, device = shape[0], self.betas.device\n\n        img = torch.randn(shape, device = device)\n        imgs = [img]\n\n        x_start = None\n        \n        ###########################################\n        ## TODO: plot the sampling process ##\n        ###########################################\n        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n            img, x_start = self.p_sample(img, t)\n            plt.show()\n        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n\n        ret = self.unnormalize(ret)\n        return ret\n\n    @torch.no_grad()\n    def sample(self, batch_size = 16, return_all_timesteps = False):\n        image_size, channels = self.image_size, self.channels\n        sample_fn = self.p_sample_loop\n        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    @property\n    def loss_fn(self):\n        return F.mse_loss\n\n\n    def p_losses(self, x_start, t, noise = None):\n        b, c, h, w = x_start.shape\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # noise sample\n\n        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n\n        # predict and take gradient step\n\n        model_out = self.model(x, t)\n\n        loss = self.loss_fn(model_out, noise, reduction = 'none')\n        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n\n        loss = loss * extract(self.loss_weight, t, loss.shape)\n        return loss.mean()\n\n    def forward(self, img, *args, **kwargs):\n        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n        img = self.normalize(img)\n        return self.p_losses(img, t, *args, **kwargs)\n","metadata":{"id":"X7TKWoZpInQs","execution":{"iopub.status.busy":"2023-04-21T04:24:46.781738Z","iopub.execute_input":"2023-04-21T04:24:46.782155Z","iopub.status.idle":"2023-04-21T04:24:47.072840Z","shell.execute_reply.started":"2023-04-21T04:24:46.782106Z","shell.execute_reply":"2023-04-21T04:24:47.071492Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Define Trainer: define the updating process","metadata":{"id":"yWJUjFIHInQt"}},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(\n        self,\n        diffusion_model,\n        folder,\n        *,\n        train_batch_size = 16,\n        gradient_accumulate_every = 1,\n        train_lr = 1e-4,\n        train_num_steps = 100000,\n        ema_update_every = 10,\n        ema_decay = 0.995,\n        adam_betas = (0.9, 0.99),\n        save_and_sample_every = 1000,\n        num_samples = 25,\n        results_folder = './results',\n        split_batches = True,\n        inception_block_idx = 2048\n    ):\n        super().__init__()\n\n        # accelerator\n\n        self.accelerator = Accelerator(\n            split_batches = split_batches,\n            mixed_precision = 'no'\n        )\n        \n\n        # model\n\n        self.model = diffusion_model\n        self.channels = diffusion_model.channels\n\n        # sampling and training hyperparameters\n\n        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n        self.num_samples = num_samples\n        self.save_and_sample_every = save_and_sample_every\n\n        self.batch_size = train_batch_size\n        self.gradient_accumulate_every = gradient_accumulate_every\n\n        self.train_num_steps = train_num_steps\n        self.image_size = diffusion_model.image_size\n\n        # dataset and dataloader\n\n        self.ds = Dataset(folder, self.image_size)\n        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n\n        dl = self.accelerator.prepare(dl)\n        self.dl = cycle(dl)\n\n        # optimizer\n\n        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n\n        # for logging results in a folder periodically\n\n        if self.accelerator.is_main_process:\n            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n            self.ema.to(self.device)\n\n        self.results_folder = Path(results_folder)\n        self.results_folder.mkdir(exist_ok = True)\n\n        # step counter state\n\n        self.step = 0\n\n        # prepare model, dataloader, optimizer with accelerator\n\n        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n\n    @property\n    def device(self):\n        return self.accelerator.device\n\n    def save(self, milestone):\n        if not self.accelerator.is_local_main_process:\n            return\n\n        data = {\n            'step': self.step,\n            'model': self.accelerator.get_state_dict(self.model),\n            'opt': self.opt.state_dict(),\n            'ema': self.ema.state_dict(),\n            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n        }\n\n        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n\n    def load(self, ckpt):\n        accelerator = self.accelerator\n        device = accelerator.device\n\n        data = torch.load(ckpt, map_location=device)\n\n        model = self.accelerator.unwrap_model(self.model)\n        model.load_state_dict(data['model'])\n\n        self.step = data['step']\n        self.opt.load_state_dict(data['opt'])\n        if self.accelerator.is_main_process:\n            self.ema.load_state_dict(data[\"ema\"])\n\n\n        if exists(self.accelerator.scaler) and exists(data['scaler']):\n            self.accelerator.scaler.load_state_dict(data['scaler'])\n\n\n    def train(self):\n        accelerator = self.accelerator\n        device = accelerator.device\n\n        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n\n            while self.step < self.train_num_steps:\n\n                total_loss = 0.\n\n                for _ in range(self.gradient_accumulate_every):\n                    data = next(self.dl).to(device)\n\n                    with self.accelerator.autocast():\n                        loss = self.model(data)\n                        loss = loss / self.gradient_accumulate_every\n                        total_loss += loss.item()\n\n                    self.accelerator.backward(loss)\n\n                accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n                pbar.set_description(f'loss: {total_loss:.4f}')\n\n                accelerator.wait_for_everyone()\n\n                self.opt.step()\n                self.opt.zero_grad()\n\n                accelerator.wait_for_everyone()\n\n                self.step += 1\n                if accelerator.is_main_process:\n                    self.ema.update()\n\n                    if self.step != 0 and self.step % self.save_and_sample_every == 0:\n                        self.ema.ema_model.eval()\n\n                        with torch.no_grad():\n                            milestone = self.step // self.save_and_sample_every\n                            batches = num_to_groups(self.num_samples, self.batch_size)\n                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n\n                        all_images = torch.cat(all_images_list, dim = 0)\n\n                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n                        \n                        self.save(milestone)\n\n                pbar.update(1)\n\n        accelerator.print('training complete')\n        \n    def inference(self, num=1000, n_iter=5, output_path='./submission'):\n        if not os.path.exists(output_path):\n            os.mkdir(output_path)\n        with torch.no_grad():\n            for i in range(n_iter):\n                batches = num_to_groups(num // n_iter, 200)\n                all_images = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))[0]\n                for j in range(all_images.size(0)):\n                    torchvision.utils.save_image(all_images[j], f'{output_path}/{i * 200 + j + 1}.jpg')              \n                ","metadata":{"id":"Ed12NNXPtDon","execution":{"iopub.status.busy":"2023-04-21T04:24:47.074849Z","iopub.execute_input":"2023-04-21T04:24:47.075630Z","iopub.status.idle":"2023-04-21T04:24:47.102797Z","shell.execute_reply.started":"2023-04-21T04:24:47.075532Z","shell.execute_reply":"2023-04-21T04:24:47.101690Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Training Hyper-parameters","metadata":{"id":"TZM7HR-UInQu"}},{"cell_type":"code","source":"path = './faces'\nIMG_SIZE = 64             # Size of images, do not change this if you do not know why you need to change\nbatch_size = 16\ntrain_num_steps = 10000        # total training steps\nlr = 1e-3\ngrad_steps = 1            # gradient accumulation steps, the equivalent batch size for updating equals to batch_size * grad_steps = 16 * 1\nema_decay = 0.995           # exponential moving average decay\n\nchannels = 16             # Numbers of channels of the first layer of CNN\ndim_mults = (1, 2, 4)        # The model size will be (channels, 2 * channels, 4 * channels, 4 * channels, 2 * channels, channels)\n\ntimesteps = 100            # Number of steps (adding noise)\nbeta_schedule = 'linear'\n\nmodel = Unet(\n    dim = channels,\n    dim_mults = dim_mults\n)\n\ndiffusion = GaussianDiffusion(\n    model,\n    image_size = IMG_SIZE,\n    timesteps = timesteps,\n    beta_schedule = beta_schedule\n)\n\ntrainer = Trainer(\n    diffusion,\n    path,\n    train_batch_size = batch_size,\n    train_lr = lr,\n    train_num_steps = train_num_steps,\n    gradient_accumulate_every = grad_steps,\n    ema_decay = ema_decay,\n    save_and_sample_every = 1000\n)\n\ntrainer.train()","metadata":{"id":"wOZPtVPvInQu","execution":{"iopub.status.busy":"2023-04-21T04:24:47.104606Z","iopub.execute_input":"2023-04-21T04:24:47.105098Z","iopub.status.idle":"2023-04-21T04:42:28.399253Z","shell.execute_reply.started":"2023-04-21T04:24:47.105041Z","shell.execute_reply":"2023-04-21T04:42:28.397649Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf12f5b86de4fea838e64317a0ad051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83aec800589543db92b0d2fec0ed4de1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d29256bdfe4808b58bd288da2440b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e217495d82f34cc584dbc8f739cb341a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03407fadfae849c1ae602aa7b336e976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6d89b6b2c54cfe873cbc51e87bc0c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b83c7acf8b4fd7afd540167777ac90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40ab3666cc15458aa197b8c4853d6eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8065ea130e8944ad8f686deeea32cf32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6db81a479748f091648633b3e2a446"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33877efe1e0e4f01baad803ffe39d699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e937eb6790e84c37bcff3a4c5b12fc85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3c04de5ee54d97b052f4667917db78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bbfc452d174a7d9beeda04fb478676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f38ba433b0d14d8bbe472cfcfae09785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b17ee97d194f37b99fc31c395e3a88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ce6666a9b448bdb3b647ea0d044b18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d89999de47fe45f5a9e3e4873ea2d333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b913baed9f4513b107ebb2da12b9cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef78eef0f234b19bf66d73762d69715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sampling loop time step:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a88b9933414d4a8a6b38137b26d5bc"}},"metadata":{}},{"name":"stdout","text":"training complete\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"SV7OL7PvInQu"}},{"cell_type":"code","source":"ckpt = '/content/drive/MyDrive/ML 2023 Spring/model-55.pt'\ntrainer.load(ckpt)\ntrainer.inference()\n","metadata":{"colab":{"referenced_widgets":["c0ff441ab9134c26bf11e280c7e36929","f2020e5f3c5241da84df29fa2c1b4d16","342ee8b0016541c682a123257e101e63","345d7222fd6d4f95833669ffe5952014","036111dd881d4a8fab3b79d2db3acdaf","c6038dd288bf4816ae9900d6c73a9e65","ea04e013b1f642dc870de1df04c57ed5","53c633cb93124bc0b0810dfd0b24baf5","8359e85df3064a19b9972efb0c643a82","368f40afa84b4790bda61731a7a41d73","aea443db2f844397850dab7678420f0f","64978c5781204244b472f6ace8562ee1","318d9d7d551749e6a0f39619df0c1b84","558d490f6d2c4c8b8e000b530461de9e","bff07d1462c641b3a0c72e64160f591d","7a7c8dc2e73a4c7b939edb0cef37f087","9d41c72c05cb480ca046db24a4934527","ac84cc26cd04421bbcddb7b5ad394f7e","c77e255a1755413b8606be67c1d4f7c4","c9542868fd8c49e68cd0830234a24e8f","cc3483424cb54909836b83e27c9c017b","0e105386196d47f3bf67ac882f3348b2","78df1721cf6b4f40b00844d24b648a94","fb0aa36f515747edbd1d9176da550698","5b81d1dc66e841528003ca25b563b129","be31d2a4f22b4a6e934c17bd54c9d30f","54368585fc164f24bb0d833f9c436057","e74fb8135c7d4f6dbeaadab046b8768a","0880bb1992b740b7bbd4b4e1ddaa2853","e8b016a35a1b47c0879ac10874de5f16","e3a961ff074949e3a51ba29c146c590a","7110474a9f194164b25633f73ec87617","bc0ddf2963684a328613cdcbd8f3741e","a1c65c89cae14d8fb021158738d6b752","671b10fb3e754dd393068e45bb890b00","9a5d9fff41c34de6ac4cebc82c466d4b","fe520552053a4d01aabc5aee75a09fe7","dd54e51c3e1e487ea683d75ecf0dc798","19cf34230bf046aa9fa2873809ce08f8","dc325ef90bb04c37a9cec6fb7ef46b36","2e835ab8b0764882a0a634ff4b2077e9","086472ea8a644f32b80008635751677e","e9c751554c2a49d3858e9643e5925260","a84156edf2514f9cbd08e403691c85eb","eec8b13273c04d03be5fdc5aecb537fa"],"base_uri":"https://localhost:8080/","height":162},"id":"MHoY_6CrInQv","outputId":"010af6c5-a426-42cd-b560-721fff3baa84","executionInfo":{"status":"ok","timestamp":1680107081454,"user_tz":-480,"elapsed":397229,"user":{"displayName":"伏宇寬","userId":"05448255760746177782"}},"execution":{"iopub.status.busy":"2023-04-21T04:42:28.402632Z","iopub.execute_input":"2023-04-21T04:42:28.403085Z","iopub.status.idle":"2023-04-21T04:42:28.462880Z","shell.execute_reply.started":"2023-04-21T04:42:28.403030Z","shell.execute_reply":"2023-04-21T04:42:28.461250Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2354255860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ML 2023 Spring/model-55.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/4166543928.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, ckpt)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ML 2023 Spring/model-55.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/ML 2023 Spring/model-55.pt'","output_type":"error"}]},{"cell_type":"code","source":"%cd ./submission\n!tar -zcf ../submission.tgz *.jpg\n%cd ..","metadata":{"id":"GkWpuU-2KzIL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680107081754,"user_tz":-480,"elapsed":302,"user":{"displayName":"伏宇寬","userId":"05448255760746177782"}},"outputId":"2215341d-4f7f-48c3-85ac-403e9ad2cb27","execution":{"iopub.status.busy":"2023-04-21T04:42:28.464225Z","iopub.status.idle":"2023-04-21T04:42:28.465058Z","shell.execute_reply.started":"2023-04-21T04:42:28.464726Z","shell.execute_reply":"2023-04-21T04:42:28.464753Z"},"trusted":true},"execution_count":null,"outputs":[]}]}